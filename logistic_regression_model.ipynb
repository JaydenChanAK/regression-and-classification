{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Overview\n",
    "Title: Logistic Regression Model\n",
    "\n",
    "Author: Jayden Chan\n",
    "\n",
    "Description: This notebook will seek to implement a logistic regression model on a dataset of credit card fraud. The dataset is comprised of several features including: distance from home, distance from last transaction, ratio to median purchase price, etc. The goal of this model will be to predict whether a credit card transaction is fraudulent or not.\n",
    "\n",
    "Credits: The dataset used in this notebook has been sourced from [Kaggle](https://www.kaggle.com/datasets/dhanushnarayananr/credit-card-fraud). Some code has been sourced from [Coursera](https://www.coursera.org/learn/machine-learning) and edited to fit project requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages\n",
    "- [numpy](www.numpy.org)\n",
    "- [matplotlib](http://matplotlib.org)\n",
    "-  ``utility.py`` contains useful helper functions for the implementation of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utility import *\n",
    "import copy\n",
    "import math\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data\n",
    "- `categories` contains the categories of the dataset.\n",
    "- `x_array` contains the features in a 2D array.\n",
    "- `y_array` contains the labels.\n",
    "    - `y_array` = 1 if the transaction was fraudulent\n",
    "    - `y_array` = 0 if the transaction was legitimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories, x_array, y_array = load_data(\"card_transdata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The categories are:\n",
      " ['distance_from_home' 'distance_from_last_transaction'\n",
      " 'ratio_to_median_purchase_price' 'repeat_retailer' 'used_chip'\n",
      " 'used_pin_number' 'online_order']\n"
     ]
    }
   ],
   "source": [
    "print(\"The categories are:\\n\", categories[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First five elements in x_array are:\n",
      " [[57.87785658  0.31114001  1.94593998  1.          1.          0.\n",
      "   0.        ]\n",
      " [10.8299427   0.1755915   1.29421881  1.          0.          0.\n",
      "   0.        ]\n",
      " [ 5.09107949  0.80515259  0.42771456  1.          0.          0.\n",
      "   1.        ]\n",
      " [ 2.24756433  5.60004355  0.36266258  1.          1.          0.\n",
      "   1.        ]\n",
      " [44.190936    0.56648627  2.2227673   1.          1.          0.\n",
      "   1.        ]]\n",
      "Type of x_array <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(\"First five elements in x_array are:\\n\", x_array[:5])\n",
    "print(\"Type of x_array\", type(x_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First five elements in y_array are:\\n\", y_array[:5])\n",
    "print(\"Type of y_array\", type(y_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('The shape of x_array is: ' + str(x_array.shape))\n",
    "print ('The shape of y_array is: ' + str(y_array.shape))\n",
    "print ('We have m = %d training examples' % (len(y_array)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Scaling\n",
    "\n",
    "We will perform feature scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_array = feature_scaling(x_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Data\n",
    "Change the `category1` and `category2` inputs to determine what is graphed on the x-axis and y-axis.\n",
    "\n",
    "Change the `x_limiter` and `y_limiter` inputs to determine how the x-axis and y-axis are scaled.\n",
    "\n",
    "Change the `marker_size` input to determine the size of markers on the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== User Inputs =====\n",
    "category1 = 0\n",
    "category2 = 2\n",
    "\n",
    "x_limiter = 0.25\n",
    "y_limiter = 0.25\n",
    "\n",
    "marker_size = 3\n",
    "\n",
    "# ===== Graph Code =====\n",
    "logistic_graph(x_array, y_array, category1, category2, marker_size, pos_label=\"Fraudulent\", neg_label=\"Legitimate\")\n",
    "\n",
    "plt.ylabel(categories[category1])\n",
    "plt.xlabel(categories[category2])\n",
    "plt.title(\"Scatter plot of training data\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "plt.xlim(0, x_limiter)\n",
    "plt.ylim(0, y_limiter)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Mapping\n",
    "\n",
    "Because the datset is nonlinear, we will have to perform feature mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original shape of data:\", x_array.shape)\n",
    "\n",
    "mapped_x = map_features(x_array)\n",
    "print(\"Shape after feature mapping:\", mapped_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"x_array[0]:\", x_array[0])\n",
    "print(\"mapped x_array[0]:\", mapped_x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we will have to account for regularization.\n",
    "\n",
    "`compute_cost` and `compute_gradients` are helper functions to implement gradient descent.\n",
    "\n",
    "`compute_cost_reg` and `compute_gradients_reg` account for regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(x, y, w, b, optional):\n",
    "    '''\n",
    "    Computes the cost function over the entire dataset.\n",
    "    \n",
    "    Args:\n",
    "        x (ndarray Shape (m,n)) : An array containing the featuries of the dataset, excluding the last column.\n",
    "        y (ndarray Shape (m,1)) : An array containing the labels of the dataset. \n",
    "        w (ndarray Shape (n,1)) : An array containing the parameters of the model.\n",
    "        b (scalar)              : The bias term.\n",
    "    Returns:\n",
    "        cost (scalar) : The total cost.\n",
    "    '''\n",
    "    \n",
    "    m = x.shape[0]\n",
    "    z_wb = np.dot(x, w) + b\n",
    "    \n",
    "    f_wb = sigmoid(z_wb)\n",
    "    \n",
    "    epsilon = 1e-10\n",
    "    f_wb = np.clip(f_wb, epsilon, 1 - epsilon)\n",
    "    \n",
    "    loss = -np.dot(y.T, np.log(f_wb)) - np.dot((1 - y).T, np.log(1 - f_wb))\n",
    "    \n",
    "    cost = (1/m)*loss\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_reg(x, y, w, b, lambda_const):\n",
    "    '''\n",
    "    Computes the cost function over the entire dataset while accounting for regularization.\n",
    "    \n",
    "    Args:\n",
    "        x (ndarray Shape (m,n))      : An array containing the featuries of the dataset, excluding the last column.\n",
    "        y (ndarray Shape (m,1))      : An array containing the labels of the dataset. \n",
    "        w (ndarray Shape (n,1))      : An array containing the parameters of the model.\n",
    "        b (scalar)                   : The bias term.\n",
    "        lambda_const (scalar, float) : Regularization constant\n",
    "    Returns:\n",
    "        cost (scalar) : The total cost.\n",
    "    '''\n",
    "    \n",
    "    m, n = x.shape\n",
    "    \n",
    "    orig_cost = compute_cost(x, y, w, b, 0)\n",
    "    reg_cost = 0\n",
    "    \n",
    "    for i in range(n):\n",
    "        reg_cost += w[i]**2\n",
    "    \n",
    "    reg_cost *= (lambda_const / (2 * m))\n",
    "    \n",
    "    cost = orig_cost + reg_cost\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(x, y, w, b, optional):\n",
    "    '''\n",
    "    Compute the gradients of the loss function with respect to the parameters w and b.\n",
    "    \n",
    "    Args:\n",
    "        x (ndarray Shape (m,n)) : An array containing the featuries of the dataset, excluding the last column.\n",
    "        y (ndarray Shape (m,1)) : An array containing the labels of the dataset. \n",
    "        w (ndarray Shape (n,1)) : An array containing the parameters of the model.\n",
    "        b (scalar)              : The bias term.\n",
    "    Returns:\n",
    "        d_dw (ndarray Shape (n,1)) : An array containing the gradients of the loss function with respect to the parameters w.\n",
    "        d_db (scalar)              : The gradient of the loss function with respect to the bias term.\n",
    "    '''\n",
    "\n",
    "    m = x.shape[0]\n",
    "    z_wb = np.dot(x, w) + b\n",
    "    \n",
    "    f_wb = sigmoid(z_wb)\n",
    "        \n",
    "    dj_dw = np.dot(x.T, f_wb - y)\n",
    "    dj_db = np.sum(f_wb - y)\n",
    "        \n",
    "    dj_dw /= m\n",
    "    dj_db /= m\n",
    "    \n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_reg(x, y, w, b, lambda_const):\n",
    "    '''\n",
    "    Compute the gradients of the loss function with respect to the parameters w and b.\n",
    "    \n",
    "    Args:\n",
    "        x (ndarray Shape (m,n))      : An array containing the featuries of the dataset, excluding the last column.\n",
    "        y (ndarray Shape (m,1))      : An array containing the labels of the dataset. \n",
    "        w (ndarray Shape (n,1))      : An array containing the parameters of the model.\n",
    "        b (scalar)                   : The bias term.\n",
    "        lambda_const (scalar, float) : Regularization constant\n",
    "    Returns:\n",
    "        d_dw (ndarray Shape (n,1)) : An array containing the gradients of the loss function with respect to the parameters w.\n",
    "        d_db (scalar)              : The gradient of the loss function with respect to the bias term.\n",
    "    '''\n",
    "\n",
    "    m, n  = x.shape\n",
    "        \n",
    "    dj_dw, dj_db = compute_gradient(x, y, w, b, 0)\n",
    "        \n",
    "    for i in range(n):\n",
    "        dj_dw_reg = (lambda_const / m) * w[i]\n",
    "        dj_dw[i] += dj_dw_reg\n",
    "    \n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`gradient_descent` allows for regularization by changing the `cost_function` and `gradient_function`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, w_i, b_i, cost_function, gradient_function, alpha, iterations, lambda_const, tolerance):\n",
    "    '''\n",
    "    Performs batch gradient descent by simultaneously updating w and b in order to reduce cost.\n",
    "    \n",
    "    Args:\n",
    "        x (ndarray Shape (m,n))      : An array containing the featuries of the dataset, excluding the last column.\n",
    "        y (ndarray Shape (m,1))      : An array containing the labels of the dataset. \n",
    "        w_i (ndarray Shape (n,1))    : An array containing the initial parameters of the model.\n",
    "        b_i (scalar)                 : The initial bias term.\n",
    "        cost_function                : Function to compute cost\n",
    "        gradient_function            : Function to compute gradient\n",
    "        alpha (float)                : Learning Rate\n",
    "        iterations (int)             : The number of times to run this function\n",
    "        lambda_const (scalar, float) : Regularization constant\n",
    "    Returns:\n",
    "        w_f (ndarray Shape (n,1)) : An array containing the updated parameters of the model.\n",
    "        b_f (scalar)              : The updated bias term.\n",
    "    '''\n",
    "    \n",
    "    cost_history = []\n",
    "    prev_cost = float('inf')\n",
    "\n",
    "    for i in range(iterations):\n",
    "        dj_dw, dj_db = gradient_function(x, y, w_i, b_i, lambda_const)\n",
    "        \n",
    "        # Simultaneous update of weights and bias\n",
    "        w_i -= alpha * dj_dw\n",
    "        b_i -= alpha * dj_db\n",
    "        \n",
    "        cost = cost_function(x, y, w_i, b_i, lambda_const)\n",
    "        cost_history.append(cost)\n",
    "        \n",
    "        if abs(cost - prev_cost) < tolerance:\n",
    "            print(f\"Convergence reached at iteration {i}. Cost {float(cost_history[-1]):8.2f}\")\n",
    "            break\n",
    "\n",
    "        prev_cost = cost\n",
    "        \n",
    "        if i % (iterations // 10) == 0 or i == (iterations-1):\n",
    "            print(f\"Iteration {i:4}: Cost {float(cost_history[-1]):8.2f}\")\n",
    "        \n",
    "    return w_i, b_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Batch Gradient Descent\n",
    "\n",
    "We will be performing regularized batch gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "m, n = x_array.shape\n",
    "w_i = 0.001 * (np.random.rand(mapped_x.shape[1]) - 0.5 )\n",
    "b_i = 10\n",
    "\n",
    "# ===== User Settings =====\n",
    "iterations = 10000\n",
    "alpha = 0.1\n",
    "lambda_const = 0.01\n",
    "tolerance = 1e-6\n",
    "\n",
    "# =========================\n",
    "w, b = gradient_descent(mapped_x, y_array, w_i, b_i, compute_cost_reg, compute_gradient_reg, \n",
    "                        alpha, iterations, lambda_const, tolerance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Decision Boundary\n",
    "\n",
    "Because there are multiple features, an n+1 dimensional graph is necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, w, b): \n",
    "    \"\"\"\n",
    "    Predict whether the label is 0 or 1.\n",
    "    \n",
    "    Args:\n",
    "        x (ndarray Shape (m,n)) : An array containing the featuries of the dataset, excluding the last column.\n",
    "        w (ndarray Shape (n,1)) : An array containing the parameters of the model.\n",
    "    Returns:\n",
    "        p (ndarray (m,1)) : The predictions for x using a threshold at 0.5\n",
    "    \"\"\"\n",
    "    \n",
    "    z_wb = np.dot(x, w) + b\n",
    "    \n",
    "    probabilities = sigmoid(z_wb)\n",
    "    \n",
    "    p = (probabilities >= 0.5).astype(int)\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = predict(mapped_x, w, b)\n",
    "\n",
    "print('Train Accuracy: %f'%(np.mean(p == y_array) * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
